# tools/generate_yolo_dataset_from_videos.py

import cv2
import random
import os
import torch

# âœ… signlang_yolo ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ (í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ëŠ” tools/)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # .../signlang_yolo/tools
ROOT_DIR = os.path.abspath(os.path.join(BASE_DIR, '..'))  # .../signlang_yolo

VIDEO_DIR = os.path.join(ROOT_DIR, 'videos')
MODEL_PATH = os.path.join(ROOT_DIR, 'models', 'best.pt')

# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = torch.hub.load('ultralytics/yolov5', 'custom', path=MODEL_PATH, force_reload=True).to('cpu')

def run_yolo_prediction(frame):
    results = model(frame)
    preds = results.xyxy[0]
    if preds is None or len(preds) == 0:
        return None
    x1, y1, x2, y2 = map(int, preds[0][:4])
    return x1, y1, x2, y2

# âœ… í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„± (0_G, 1_N ë“±)
class_list = sorted(set([
    '_'.join(os.path.splitext(f)[0].split('_')[:2])
    for f in os.listdir(VIDEO_DIR)
    if f.endswith('.mp4')
]))
class_to_index = {name: idx for idx, name in enumerate(class_list)}

print("ğŸ’¡ YOLO ë°ì´í„°ì…‹ ìë™ ìƒì„± ì‹œì‘")
print(f"ğŸ§¾ í´ë˜ìŠ¤ ë§¤í•‘: {class_to_index}")

# âœ… ë¹„ë””ì˜¤ ë°˜ë³µ ì²˜ë¦¬
for video_file in os.listdir(VIDEO_DIR):
    if not video_file.endswith('.mp4'):
        continue

    label_candidate = '_'.join(os.path.splitext(video_file)[0].split('_')[:2])
    if label_candidate not in class_to_index:
        print(f"âš  '{video_file}' â†’ ë¼ë²¨ ì—†ìŒ, ìŠ¤í‚µ")
        continue

    label_index = class_to_index[label_candidate]
    cap = cv2.VideoCapture(os.path.join(VIDEO_DIR, video_file))

    frame_count = 0
    saved_frame_count = 0

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame = cv2.resize(frame, (640, 640))
            bbox = run_yolo_prediction(frame)
            if bbox is None:
                print(f"âš  ì† ë¯¸íƒì§€: {video_file} - frame {frame_count}")
                frame_count += 1
                continue

            h, w, _ = frame.shape
            x1, y1, x2, y2 = bbox
            x_center = (x1 + x2) / 2 / w
            y_center = (y1 + y2) / 2 / h
            width = (x2 - x1) / w
            height = (y2 - y1) / h

            # âœ… train/val ë¶„ë¦¬
            if random.random() < 0.8:
                image_dir = os.path.join(ROOT_DIR, 'datasets/images/train')
                label_dir = os.path.join(ROOT_DIR, 'datasets/labels/train')
            else:
                image_dir = os.path.join(ROOT_DIR, 'datasets/images/val')
                label_dir = os.path.join(ROOT_DIR, 'datasets/labels/val')

            os.makedirs(image_dir, exist_ok=True)
            os.makedirs(label_dir, exist_ok=True)

            image_name = f"{label_candidate}_{frame_count}.jpg"
            label_name = f"{label_candidate}_{frame_count}.txt"

            cv2.imwrite(os.path.join(image_dir, image_name), frame)
            with open(os.path.join(label_dir, label_name), 'w') as f:
                f.write(f"{label_index} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")

            frame_count += 1
            saved_frame_count += 1

    except KeyboardInterrupt:
        print("ğŸ›‘ ì¤‘ë‹¨ë¨ (í˜„ì¬ê¹Œì§€ ì €ì¥ëœ í”„ë ˆì„ ìœ ì§€)")

    cap.release()
    print(f"âœ… {video_file} ì²˜ë¦¬ ì™„ë£Œ - {saved_frame_count} í”„ë ˆì„ ì €ì¥ë¨")

print("ğŸ‰ YOLO í•™ìŠµìš© ë°ì´í„°ì…‹ ìë™ ë³€í™˜ ì™„ë£Œ!")
